{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first idea for this challenge is to create a subtracted image at each interval, and plug that into a CNN that in turn regresses onto the speed of the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('data/train.mp4')\n",
    "train_speeds = np.loadtxt('data/train.txt')\n",
    "sec = 0\n",
    "frameRate = 1/20 #//it will capture image in each 0.5 second\n",
    "count=0\n",
    "#success = getFrame(sec)\n",
    "imgs = []\n",
    "img_diff = []\n",
    "hasFrames = True\n",
    "while count < 2040:\n",
    " #    print(count)\n",
    "    count = count + 1\n",
    "    sec = sec + frameRate\n",
    "   # sec = round(sec, 2)\n",
    "  #  vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n",
    "    hasFrames,image = vidcap.read()\n",
    "    imgs.append(image)\n",
    "    if count == 0:\n",
    "        #what should the first value be? \n",
    "        print(\"Skipped the first frame, and will do the same in the training set. \")\n",
    "    if count > 1:\n",
    "        img_diff.append(imgs[count-1] - imgs[count - 2])\n",
    "\n",
    "        \n",
    "#img diff as a pytorch tensor??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_diff = np.array(img_diff)\n",
    "train_speeds = train_speeds[1:len(img_diff)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader... Batch size needs to be designated now too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(img_diff,train_speeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda3/lib/python3.7/site-packages/torch/tensor.py:330: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f56d932b8d01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_loader = []\n",
    "x = 0\n",
    "while x < len(X_train) // batch_size:\n",
    "    Xtrain_batch = []\n",
    "    ytrain_batch = []\n",
    "    i = 0\n",
    "    while i < batch_size:\n",
    "        Xtrain_batch.append(X_train[i])\n",
    "        ytrain_batch.append(y_train[i])\n",
    "        i +=1\n",
    "        \n",
    "    train_loader.append((torch.Tensor(Xtrain_batch).resize(batch_size,3,480,640),torch.Tensor(ytrain_batch)))\n",
    "    X_train = X_train[i:]\n",
    "    y_train = y_train[i:]\n",
    "    x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "val_loader = []\n",
    "x = 0\n",
    "while x < len(X_val) // batch_size:\n",
    "    Xval_batch = []\n",
    "    yval_batch = []\n",
    "    i = 0\n",
    "    while i < batch_size:\n",
    "        Xval_batch.append(X_val[i])\n",
    "        yval_batch.append(y_val[i])\n",
    "        i +=1\n",
    "        \n",
    "    val_loader.append((torch.Tensor(Xval_batch).resize(batch_size,3,480,640),torch.Tensor(yval_batch)))\n",
    "    X_val = X_val[i:]\n",
    "    y_val = y_val[i:]\n",
    "    x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import pytorch, and prepare a model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,              # input height\n",
    "                out_channels=16,            # n_filters\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after con2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (16, 28, 28)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 14, 14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     # output shape (32, 14, 14)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(2),                # output shape (32, 7, 7)\n",
    "        ) \n",
    "        \n",
    "        self.out = nn.Linear(32 * 120 * 160, 1)   # fully connected layer, output 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        print(\"Shape after conv 2 \", x.shape)\n",
    "        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        output = self.out(x)\n",
    "        return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the network: 628449\n"
     ]
    }
   ],
   "source": [
    "# define the loss and optimizer\n",
    "#instantiate the model and quickly state how many parameters it has \n",
    "model =  CNN()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of parameters in the network: %d'%total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss and optimizer\n",
    "criterion = nn.MSELoss()  #mean squared error is good for regression :)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, \n",
    "                             weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "# create the training, validation and testing datasets\n",
    "\n",
    "\n",
    "print(\"data is loaded in ! \")\n",
    "\n",
    "\n",
    "\n",
    "#for weight decay \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, \n",
    "                                                       patience=10, verbose=True)\n",
    "\n",
    "\n",
    "model.eval()   #what's this do \n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    count, loss_train = 0, 0.0\n",
    "    #loader contains dark matter cube, dm subhalo cube, subhalo + galaxy cube, dm + galaxy mask, and coords. \n",
    "    for maps,  params_true in train_loader: #currently ignoring subhalo structure. \n",
    "        maps = maps.to(device).float()\n",
    "        params_true = params_true.to(device).float()\n",
    "        # Forward Pass\n",
    "        optimizer.zero_grad()\n",
    "        params_pred = model(maps)\n",
    "        loss = criterion(params_pred, params_true)\n",
    "        loss_train += loss.cpu().detach().numpy()\n",
    "        \n",
    "        # Backward Prop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "    loss_train /= count\n",
    "    \n",
    "    # VALID\n",
    "    model.eval() \n",
    "    count, loss_valid = 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for maps, params_true  in val_loader:\n",
    "            \n",
    "            maps = maps.to(device).float()\n",
    "            params_true = params_true.to(device).float()\n",
    "            params_pred = model(maps)\n",
    "            error    = criterion(params_pred, params_true)   \n",
    "            loss_valid += error.cpu().numpy()\n",
    "            count += 1\n",
    "    loss_valid /= count\n",
    "     \n",
    "        \n",
    "    print('%03d %.4e %.4e'%(epoch, loss_train, loss_valid))\n",
    "    # update learning rate\n",
    "    scheduler.step(loss_valid)\n",
    "    # save results to file\n",
    "    f = open(fout, 'a')\n",
    "    f.write('%d %.4e %.4e %.4e\\n'%(epoch, loss_train, loss_valid, loss_test))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda3/lib/python3.7/site-packages/torch/tensor.py:330: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "X_in = torch.Tensor(X_train[0:10]).resize(10,3,480,640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need some way to confirm the amount of samples I'm taking is the right amount. Frame rate data/train.mp4 is a video of driving containing 20400 frames. Video is shot at 20 fps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after conv 2  torch.Size([10, 32, 120, 160])\n"
     ]
    }
   ],
   "source": [
    "out = model.forward(X_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps:\n",
    "\n",
    "Tell image diff it belongs in a training and validation dataset, make it split in a time based way though! Only sample last part or something.\n",
    "\n",
    "\n",
    "Make a model to use, this will be MSE on a single value, start with some vanilla CNN. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
